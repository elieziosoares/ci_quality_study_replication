{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate Pull Requests data\n",
    "Process commits data to summarize values from each pull request.\n",
    "\n",
    "1. Run PRCommitMiner to mine PR commits and store data on the database.\n",
    "2. saveCommitsResults(pr_number,merge_commit_sha,qty_commits)\n",
    "    - Process commits info\n",
    "2. GitHub web scrap to complement data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~1. Consolidating data from individual commits on pull requests~\n",
    "\n",
    "- ~__[Step Removed]__ First, look to merge commits.~\n",
    "    - ~Lastly, get individual commits.~\n",
    "        - ~If the quantity of mined commits is equals to commits in the pullrequest: Consolidate them.~\n",
    "        - ~If not, flag the PR to the next phase (incompletecommits = True)~\n",
    "    - ~Scrap GitHub pages for each incomplete Pull Request.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T09:22:01.546562Z",
     "start_time": "2021-10-23T02:22:57.200201Z"
    }
   },
   "outputs": [],
   "source": [
    "projects = getProjects()\n",
    "\n",
    "for repo in projects:\n",
    "    print('\\n\\n----------- {} - INITIALIZING {} -----------\\n\\n'.format(datetime.now().strftime(\"%H:%M:%S\"),repo[0]))\n",
    "    \n",
    "    #collect pull requests for each repo\n",
    "    PRs = getPRs(repo)\n",
    "    for PR in PRs:\n",
    "            #Verify tables commits and merge_commits searching for results \n",
    "            #of mining process to save processed results by pullrequest\n",
    "        saveCommitsResults(repo,PR[0],PR[1], PR[2])\n",
    "\n",
    "    print('\\n\\n----------- {}'.format(datetime.now().strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~GitHub Web scraping to complement pull requests data~\n",
    "### GitHub API requests to complement pull requests data\n",
    "\n",
    "* ~Pull requests that was not mined neither by merge commit or by list of commits~\n",
    "* This scrapping process was swapped by a new endpoint API:\n",
    "   * https://api.github.com/repos/{owner}/{repo}/pulls/{pull_number}/files\n",
    "  \n",
    "  \n",
    "##### We also compute the author frequency:\n",
    "* Author_frequency = Ncommits / Nauthors\n",
    "* Where:\n",
    "   * Ncommits: Number of commits in a pull request.\n",
    "   * Nauthors: Number of distinct authors in a pull request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRs = getPRsToMine()\n",
    "i=1\n",
    "for PR in PRs:\n",
    "    project =  PR[0]\n",
    "    pull =  PR[1]\n",
    "    \n",
    "    print(f\"-- {i}/{len(PRs)}: Project {project} - PR {pull}\")    \n",
    "    i += 1\n",
    "    set_changed_lines(project, pull)\n",
    "    set_author_frequency(project,pull)\n",
    "    \n",
    "    \n",
    "print('\\n\\n----------- {} - PROCESS FINISHED -----------\\n\\n'.format(datetime.now().strftime(\"%D/%M/%Y %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_author_frequency(repo, pull_number):\n",
    "    author_frequency_data = getPRAuthorFrequency(repo, pull_number)\n",
    "    \n",
    "    if author_frequency_data is None or author_frequency_data[1] == 0:\n",
    "        commits = 1\n",
    "    else:\n",
    "        commits = author_frequency_data[1]\n",
    "        \n",
    "    if author_frequency_data is None or author_frequency_data[2] == 0:\n",
    "        authors = 1\n",
    "    else:\n",
    "        authors = author_frequency_data[2]\n",
    "    \n",
    "    author_frequency = commits / authors\n",
    "    \n",
    "    print(f\"\\t Commits:{commits}, Authors: {authors}, Author Frequency: {author_frequency}\")\n",
    "    #update PR\n",
    "    updateAuthorFrequency(repo,pull_number,author_frequency,authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commits = getPRCommits('anastr/SpeedView', 81)\n",
    "#len(commits)\n",
    "authors = getPRAuthorFrequency('apache/shardingsphere-elasticjob', 1344)\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_changed_lines(repo, pull_number):\n",
    "    # Faz uma requisição para obter as alterações de um commit específico da pull request\n",
    "    url = f\"https://api.github.com/repos/{repo}/pulls/{pull_number}/files\"\n",
    "    response = requestAPI(url)\n",
    "    \n",
    "    commit_size = added = removed = files = test_files = test_lines = file_name =0\n",
    "    \n",
    "    if response is None:\n",
    "        return None\n",
    "    \n",
    "    files = len(response)\n",
    "    # Itera sobre os arquivos alterados e obtém as linhas adicionadas e removidas\n",
    "    for file in response:\n",
    "        added += file['additions']\n",
    "        removed += file['deletions']\n",
    "        commit_size += file['changes']\n",
    "        \n",
    "        file_name = file['filename']\n",
    "        \n",
    "        if isTestFile(file_name):\n",
    "            test_files += 1\n",
    "            test_lines += added + removed\n",
    "        \n",
    "    \n",
    "    print(f\"\\t Size:{commit_size}, Added: {added}, removed: {removed}, files: {files}, test files: {test_files}, test lines: {test_lines}\")\n",
    "    #update PR\n",
    "    updatePRApi(repo,pull_number,removed,added,files,commit_size,test_lines,test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-25T02:20:07.528Z"
    }
   },
   "outputs": [],
   "source": [
    "PRs = getPRsToScrap()\n",
    "for PR in PRs:\n",
    "    url = 'https://github.com/'+ PR[0]+'/pull/'+str(PR[1])+'/files'\n",
    "    print(url)\n",
    "    \n",
    "    response = requestPage(url)\n",
    "    if response is not None:\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        scrapPR(html_soup,PR[0],PR[1])\n",
    "    #sleep\n",
    "    time.sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.778038Z",
     "start_time": "2021-10-25T02:19:54.488699Z"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import requests \n",
    "import time\n",
    "import pytz    \n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.792174Z",
     "start_time": "2021-10-25T02:19:55.782343Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPRsToScrap():\n",
    "    #query = \"\"\"select project_name, pr_number from PULLREQUESTS \n",
    "    #WHERE incompletecommits is true\n",
    "    #order by project_name, pr_number\n",
    "    #offset 0 limit 5004\"\"\"\n",
    "    \n",
    "    query = \"\"\"SELECT project_name, pr_number from PULLREQUESTS\n",
    "                WHERE (commit_size is null or incompletecommits is true) and release_id IN (\n",
    "                    SELECT R.node_id FROM PROJECTS P\n",
    "                        INNER JOIN PROJECT_RELEASES R ON P.repo_name = R.repo_name\n",
    "                    WHERE P.analysis_releases > 0 AND P.analysis_issues > 0 AND P.analysis_prs > 0\n",
    "                        AND R.created_at between P.analysis_init AND P.analysis_finish\n",
    "                    )\n",
    "                    order by project_name, pr_number\"\"\"\n",
    "    \n",
    "    connection = connectDB()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPRsToMine():\n",
    "    query = \"\"\"SELECT project_name, pr_number from metrics_releases m \n",
    "                inner join pullrequests p on m.release_id = p.release_id\n",
    "                order by project_name, pr_number\"\"\"\n",
    "    \n",
    "    connection = connectDB()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.803971Z",
     "start_time": "2021-10-25T02:19:55.794800Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrapPR(html_soup,repo_name,pr_number):\n",
    "    path = '/home/eliezio/Doutorado/Causalidade/PR_scraping/'+repo_name.replace('/','_')+'--'+str(pr_number)\n",
    "    \n",
    "    commit_size = added = removed = files = test_files = test_lines = file_name =0\n",
    "\n",
    "    files_soup = html_soup.select('.file')\n",
    "    for file_s in files_soup:\n",
    "        files += 1 \n",
    "\n",
    "        try:\n",
    "            commit_size += getCommitSize(file_s)\n",
    "            added += getAddedLines(file_s)\n",
    "            removed += getRemovedLines(file_s)\n",
    "        except Exception as e:\n",
    "            print('--- Exception: {}'.format(e))\n",
    "            print('--- Error when scraping changes. Keep walking.')\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            file_name = file_s.select('.Link--primary')[0]['title']\n",
    "            if isTestFile(file_name):\n",
    "                test_files += 1\n",
    "                test_lines += getAddedLines(file_s) + getRemovedLines(file_s)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('--- Exception: {}'.format(e))\n",
    "            print('--- Error when scraping file name and test info. Keep walking.')\n",
    "    \n",
    "    #save the file for future usage.\n",
    "    f = open(path, 'w')\n",
    "    f.write(html_soup.prettify())\n",
    "    f.close()\n",
    "    \n",
    "    #update PR\n",
    "    updatePRscrap(repo_name,pr_number,removed,added,files,commit_size,test_lines,test_files)\n",
    "\n",
    "    print('Project: {}; PR number: {}; Total Commit size: {}; Added: {}; Removed: {}; Files: {}; test files: {}; test_lines: {}'.format(repo_name,pr_number,commit_size,added,removed, files,test_files,test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.811353Z",
     "start_time": "2021-10-25T02:19:55.805712Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCommitSize(file_s):\n",
    "    diff_text = file_s.select('.diffstat')\n",
    "    diff_text = str(diff_text[0]['aria-label'])\n",
    "    diff_text = diff_text.rsplit(' ')\n",
    "    \n",
    "    return int(diff_text[0].replace(',',''))\n",
    "\n",
    "def getAddedLines(file_s):\n",
    "    diff_text = file_s.select('.diffstat')\n",
    "    diff_text = str(diff_text[0]['aria-label'])\n",
    "    diff_text = diff_text.rsplit(' ')\n",
    "    \n",
    "    return int(diff_text[2].replace(',',''))\n",
    "\n",
    "def getRemovedLines(file_s):\n",
    "    diff_text = file_s.select('.diffstat')\n",
    "    diff_text = str(diff_text[0]['aria-label'])\n",
    "    diff_text = diff_text.rsplit(' ')\n",
    "    \n",
    "    return int(diff_text[5].replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "test = 'test/e2e/gridObjectTestUtils.spec.js'\n",
    "print(isTestFile(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.816358Z",
     "start_time": "2021-10-25T02:19:55.812848Z"
    }
   },
   "outputs": [],
   "source": [
    "def isTestFile(file):\n",
    "    if '_test' in file or 'test_' in file or 'Test_' in file or '_Test' in file or '_TEST' in file or '_TEST' in file:\n",
    "        if not 'latest' in file and not 'LATEST' in file:\n",
    "            return True;\n",
    "    return False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.821490Z",
     "start_time": "2021-10-25T02:19:55.817828Z"
    }
   },
   "outputs": [],
   "source": [
    "def connectDB():\n",
    "    f = open('/home/psql_pwd.txt', \"r\")\n",
    "    pwd = f.readline().replace('\\n','')\n",
    "    \n",
    "    return psycopg2.connect(user = \"ci_quality\",\n",
    "                              password = pwd,\n",
    "                              host = \"127.0.0.1\",\n",
    "                              port = \"5432\",\n",
    "                              database = \"Causal_CI_Quality_v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.827188Z",
     "start_time": "2021-10-25T02:19:55.823108Z"
    }
   },
   "outputs": [],
   "source": [
    "def saveCommitsResults(repo_name,pr_number,merge_commit_sha, commits):\n",
    "    #get mined merge commits \n",
    "    merge_commit = getMergeCommit(merge_commit_sha)\n",
    "    \n",
    "    #Is there commit merge analysis?\n",
    "    #if len(merge_commit) > 0:\n",
    "    #    print('+ Merge commit found.')\n",
    "    #    updatePR(repo_name,pr_number,list(merge_commit[0]), True)\n",
    "    \n",
    "    #If there is no merge analysis, search for commits individually\n",
    "#else:\n",
    "    print('Searching for the set of commits.')\n",
    "    commits_list = getPRCommits(repo_name,pr_number)\n",
    "    #print(commits_list)\n",
    "\n",
    "    # Is there a set of commits?\n",
    "    # Is the list equivalent to PR size?\n",
    "    if len(commits_list) > 0 and len(commits_list) == commits:     \n",
    "        data = sumMatrix(list(commits_list))\n",
    "        updatePR(repo_name,pr_number,data, False)\n",
    "\n",
    "    #If not, flag this PR as incomplete.\n",
    "    else:\n",
    "        updatePRIncomplete(repo_name,pr_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.834415Z",
     "start_time": "2021-10-25T02:19:55.830133Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPRs(project_name):\n",
    "    query = \"\"\"select pr_number,merge_commit_sha,commits from PULLREQUESTS \n",
    "    where project_name like %s;\"\"\"# and commit_size is null;\"\"\"\n",
    "    \n",
    "    connection = connectDB()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query, [project_name])\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.840173Z",
     "start_time": "2021-10-25T02:19:55.836223Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPRCommits(repo_name,pr_number):\n",
    "    query = \"\"\"SELECT deletions, insertions, files, commit_size, test_volume, test_files \n",
    "    FROM COMMIT_PR CPR INNER JOIN COMMITS C ON C.commit_sha = CPR.commit_sha\n",
    "    WHERE commit_size is not NULL AND project_name like %s AND pr_number = %s\"\"\"\n",
    "\n",
    "    connection = connectDB()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query, [repo_name,pr_number])\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPRAuthorFrequency(repo_name,pr_number):\n",
    "    query = \"\"\"select p.pr_number,p.commits,count(distinct author_id) from pullrequests p\n",
    "                INNER JOIN commit_pr cp ON p.pr_number = cp.pr_number and p.project_name = cp.project_name\n",
    "                INNER JOIN commits c ON cp.COMMIT_SHA = c.COMMIT_SHA\n",
    "                where cp.pr_number = %s and cp.project_name like %s\n",
    "                group by p.pr_number,p.commits;\"\"\"\n",
    "\n",
    "    connection = connectDB()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query, [pr_number,repo_name])\n",
    "    result = cursor.fetchone()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.845892Z",
     "start_time": "2021-10-25T02:19:55.842135Z"
    }
   },
   "outputs": [],
   "source": [
    "def getMergeCommit(sha):\n",
    "    query = \"\"\"select deletions, insertions, files, commit_size, test_volume, test_files \n",
    "    FROM MERGE_COMMITS WHERE commit_size is not NULL AND commit_sha like %s\"\"\"\n",
    "\n",
    "    connection = connectDB()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query, [sha])\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.851642Z",
     "start_time": "2021-10-25T02:19:55.847749Z"
    }
   },
   "outputs": [],
   "source": [
    "def getProjects():\n",
    "    query = \"\"\"SELECT DISTINCT repo_name From PROJECTS \n",
    "    WHERE commits_mined IS TRUE and analysis_releases > 0 and analysis_issues > 0 and analysis_prs > 0\n",
    "    order by repo_name desc;\"\"\"\n",
    "\n",
    "    connection = connectDB()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    connection.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.863022Z",
     "start_time": "2021-10-25T02:19:55.853823Z"
    }
   },
   "outputs": [],
   "source": [
    "def updatePR(repo_name,pr_number,data,isMerge):\n",
    "    try:\n",
    "        query = \"\"\"UPDATE  PULLREQUESTS \n",
    "            set deletions = %s,\n",
    "            insertions = %s,\n",
    "            files = %s,\n",
    "            commit_size = %s,\n",
    "            test_volume = %s,\n",
    "            test_files = %s,\n",
    "            \"dataFromMergeCommit\" = %s,\n",
    "            incompletecommits = False\n",
    "            WHERE project_name like %s and pr_number = %s;\"\"\"\n",
    "\n",
    "        print(\"\"\"....Updating pullrequest: UPDATE  PULLREQUESTS set deletions = {},insertions = {},files = {},commit_size = {},test_volume = {},test_files = {}, dataFromMergeCommit = {} WHERE project_name like {} and pr_number = {};\"\"\".format(data[0],data[1],data[2],data[3],data[4],data[5],isMerge,repo_name,pr_number))\n",
    "        \n",
    "        connection = connectDB()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query, [data[0],data[1],data[2],data[3],data[4],data[5],isMerge,repo_name,pr_number])\n",
    "        connection.commit()\n",
    "        connection.close()\n",
    "    except psycopg2.IntegrityError as e:\n",
    "        print (\"==============================================================\")\n",
    "        print (\"Error while updating into PostgreSQL. updatePR >>> Exception: {}\".format(e)) \n",
    "        print('Project: {}    PR - {} '.format(repo_name, pr_number))\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print (\"==============================================================\")\n",
    "        print (\"Error while processing updatePR >>> Exception: {}\".format(e)) \n",
    "        print('Project: {}    PR - {}  '.format(repo_name, pr_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.872241Z",
     "start_time": "2021-10-25T02:19:55.864975Z"
    }
   },
   "outputs": [],
   "source": [
    "def updatePRscrap(repo_name,pr_number,removed,added,files,commit_size,test_lines,test_files):\n",
    "    try:\n",
    "        query = \"\"\"UPDATE  PULLREQUESTS \n",
    "            set deletions = %s,\n",
    "            insertions = %s,\n",
    "            files = %s,\n",
    "            commit_size = %s,\n",
    "            test_volume = %s,\n",
    "            test_files = %s,\n",
    "            datafromscrap = True,\n",
    "            \"dataFromMergeCommit\" = False,\n",
    "            incompletecommits = False\n",
    "            WHERE project_name like %s and pr_number = %s;\"\"\"\n",
    "\n",
    "        connection = connectDB()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query, [removed,added,files,commit_size,test_lines,test_files,repo_name,pr_number])\n",
    "        connection.commit()\n",
    "    except Exception as e:\n",
    "        print (\"==============================================================\")\n",
    "        print (\"Error while updatePRscrap >>> Exception: {}\".format(e)) \n",
    "        print('Project: {}    PR - {}  '.format(repo_name, pr_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePRApi(repo_name,pr_number,removed,added,files,commit_size,test_lines,test_files):\n",
    "    try:\n",
    "        query = \"\"\"UPDATE  PULLREQUESTS \n",
    "            set deletions = %s,\n",
    "            insertions = %s,\n",
    "            files = %s,\n",
    "            commit_size = %s,\n",
    "            test_volume = %s,\n",
    "            test_files = %s,\n",
    "            datafromscrap = False,\n",
    "            datafromapi = True,\n",
    "            \"dataFromMergeCommit\" = False,\n",
    "            incompletecommits = False\n",
    "            WHERE project_name like %s and pr_number = %s;\"\"\"\n",
    "\n",
    "        connection = connectDB()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query, [removed,added,files,commit_size,test_lines,test_files,repo_name,pr_number])\n",
    "        connection.commit()\n",
    "    except Exception as e:\n",
    "        print (\"==============================================================\")\n",
    "        print (\"Error while updatePRApi >>> Exception: {}\".format(e)) \n",
    "        print('Project: {}    PR - {}  '.format(repo_name, pr_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateAuthorFrequency(repo_name,pr_number,author_frequency,authors):\n",
    "    try:\n",
    "        query = \"\"\"UPDATE  PULLREQUESTS \n",
    "            set author_frequency = %s,\n",
    "            nauthors = %s\n",
    "            WHERE project_name like %s and pr_number = %s;\"\"\"\n",
    "\n",
    "        connection = connectDB()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query, [author_frequency,authors,repo_name,pr_number])\n",
    "        connection.commit()\n",
    "    except Exception as e:\n",
    "        print (\"==============================================================\")\n",
    "        print (\"Error while updateAuthorFrequency >>> Exception: {}\".format(e)) \n",
    "        print('Project: {}    PR - {}  '.format(repo_name, pr_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.881023Z",
     "start_time": "2021-10-25T02:19:55.874205Z"
    }
   },
   "outputs": [],
   "source": [
    "def updatePRIncomplete(repo_name,pr_number):\n",
    "    try:\n",
    "        query = \"\"\"UPDATE  PULLREQUESTS \n",
    "            set \"dataFromMergeCommit\" = False,\n",
    "            incompletecommits = True\n",
    "            WHERE project_name like %s and pr_number = %s;\"\"\"\n",
    "\n",
    "        print(\"\"\"....Updating INCOMPLETE pullrequest: UPDATE  PULLREQUESTS set\"dataFromMergeCommit\" = False, incompleteCommits = True WHERE project_name like {} and pr_number = {};\"\"\".format(repo_name,pr_number))\n",
    "        \n",
    "        connection = connectDB()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query, [repo_name,pr_number])\n",
    "        connection.commit()\n",
    "        connection.close()\n",
    "    except psycopg2.IntegrityError as e:\n",
    "        print (\"==============================================================\")\n",
    "        print (\"Error while updating into PostgreSQL. updatePRIncomplete >>> Exception: {}\".format(e)) \n",
    "        print('Project: {}    PR - {} '.format(repo_name, pr_number))\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print (\"==============================================================\")\n",
    "        print (\"Error while processing updatePRIncomplete >>> Exception: {}\".format(e)) \n",
    "        print('Project: {}    PR - {}  '.format(repo_name, pr_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.889203Z",
     "start_time": "2021-10-25T02:19:55.883026Z"
    }
   },
   "outputs": [],
   "source": [
    "def sumMatrix(matrix):\n",
    "    result = []\n",
    "    for c in range(len(matrix[0])):\n",
    "        res = 0\n",
    "        for v in range(len(matrix)):\n",
    "            res += int(matrix[v][c])\n",
    "            \n",
    "        result.append(res)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T02:19:55.899293Z",
     "start_time": "2021-10-25T02:19:55.891744Z"
    }
   },
   "outputs": [],
   "source": [
    "def requestPage(URL):      \n",
    "    http_proxy  = \"http://157.100.58.60:999\"\n",
    "    https_proxy = \"https://157.100.58.60:999\"\n",
    "    proxyDict = {\n",
    "        \"http\"  : http_proxy, \n",
    "        \"https\" : https_proxy\n",
    "    }\n",
    "\n",
    "    headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\"    \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #r = requests.get(url=URL, headers=headers, proxies=proxyDict)\n",
    "        r = requests.get(url=URL, headers=headers)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print('{} -- {} -- Code: {}'.format(datetime.now().strftime(\"%H:%M:%S\"),URL,r.status_code))\n",
    "            if r.status_code != 404 and r.status_code != 404:\n",
    "                time.sleep(10)\n",
    "                return requestPage(URL)\n",
    "        else:\n",
    "            return r\n",
    "    except Exception as e:\n",
    "        print('\\n Erro no request get: {}'.format(e))\n",
    "        time.sleep(10)\n",
    "        return requestPage(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTokens():\n",
    "    f = open('/home/gh_tokens.txt', \"r\")\n",
    "    tokens =[]\n",
    "    tk = f.readline().replace('\\n','')\n",
    "    while tk != '':\n",
    "        tokens.append(tk)\n",
    "        tk = f.readline().replace('\\n','')\n",
    "\n",
    "    f.close()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = loadTokens()\n",
    "i_token =0\n",
    "\n",
    "\n",
    "def requestAPI(URL):\n",
    "    # api-endpoint \n",
    "    global i_token\n",
    "    \n",
    "    if (len(tokens)-1) > i_token:\n",
    "        i_token += 1\n",
    "    else:\n",
    "        i_token = 0\n",
    "        \n",
    "    r = requests.get(url = URL, headers={'Authorization': tokens[i_token],'Accept':'application/vnd.github.cloak-preview'}).json()\n",
    "    if len(r) > 0:\n",
    "        try:\n",
    "            #print('keys: {}'.format(r.keys()))\n",
    "            \n",
    "            if isinstance(r, dict) and 'message' in r.keys():\n",
    "                print('{} -- {}'.format(datetime.now().strftime(\"%H:%M:%S\"),r['message']))\n",
    "                if 'API rate limit exceeded' in r['message']:\n",
    "                    time.sleep(600)\n",
    "                    requestAPI(URL)\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            print('\\n Erro no request get: {}'.format(e))\n",
    "            print(r)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
